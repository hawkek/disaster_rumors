{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca965806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd, numpy as np\n",
    "import tweepy\n",
    "import random\n",
    "\n",
    "from configparser import RawConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d448c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create a dataframe combining all the credibility-scored tweets that are broken dowen by topic.\n",
    "# first, create an empty dataframe that will hold each individual tweet and relevant attributes\n",
    "df_tweets = pd.DataFrame(columns = ['tweet_id','user_id','create_time','topic_key','is_credible'])\n",
    "\n",
    "# use a for-loop to read data from CSVs in topic_tweets subfolder\n",
    "# and concatenate them to the 'df_tweets' dataframe\n",
    "for i in range(224):\n",
    "    df_topic = pd.read_csv(f'../datasets/topic_tweets/topic_{i}.csv')\n",
    "    df_tweets = pd.concat([df_tweets, df_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c11a4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.665757\n",
       "0    0.334243\n",
       "Name: is_credible, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the class distribtuion for full dataset\n",
    "# about 66% are classified as true, 34% rumours.\n",
    "df_tweets.is_credible.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb80148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full dataset is too large for us to work with given computational limitations and time contraints.\n",
    "# feel free to continue the analysis using the full dataset in df_tweets,\n",
    "# or otherwise you can follow these steps to work with a subset of the data.\n",
    "# note, however, that you will get a different set of randomly generated numbers here\n",
    "# so your final dataset will contain different tweets.\n",
    "# we use a dataset containing 1.5 million tweets\n",
    "random_indices = random.sample(range(1,df_tweets.shape[0]),1500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549f582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>create_time</th>\n",
       "      <th>topic_key</th>\n",
       "      <th>is_credible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61074</th>\n",
       "      <td>555906559755452417</td>\n",
       "      <td>LoveDeedums</td>\n",
       "      <td>2015-01-16 01:57:06</td>\n",
       "      <td>birthday_king_martin-20150115_170445-20150115_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56209</th>\n",
       "      <td>562882182310809600</td>\n",
       "      <td>pomaikai44</td>\n",
       "      <td>2015-02-04 07:55:44</td>\n",
       "      <td>plane_crash_taiwan-20150204_062513-20150204_07...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98170</th>\n",
       "      <td>552866326331797504</td>\n",
       "      <td>MAZANDARA</td>\n",
       "      <td>2015-01-07 16:36:18</td>\n",
       "      <td>#charliehebdo_paris_attack-20150107_072714-201...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8782</th>\n",
       "      <td>565678607461523457</td>\n",
       "      <td>Balshe79</td>\n",
       "      <td>2015-02-12 01:07:44</td>\n",
       "      <td>#chapelhillshooting_were_media-20150211_142604...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191070</th>\n",
       "      <td>557376485049511936</td>\n",
       "      <td>nataliapresli</td>\n",
       "      <td>2015-01-20 03:18:04</td>\n",
       "      <td>king_martin_luther-20150119_130240-20150119_13...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id        user_id          create_time  \\\n",
       "61074   555906559755452417    LoveDeedums  2015-01-16 01:57:06   \n",
       "56209   562882182310809600     pomaikai44  2015-02-04 07:55:44   \n",
       "98170   552866326331797504      MAZANDARA  2015-01-07 16:36:18   \n",
       "8782    565678607461523457       Balshe79  2015-02-12 01:07:44   \n",
       "191070  557376485049511936  nataliapresli  2015-01-20 03:18:04   \n",
       "\n",
       "                                                topic_key is_credible  \n",
       "61074   birthday_king_martin-20150115_170445-20150115_...           1  \n",
       "56209   plane_crash_taiwan-20150204_062513-20150204_07...           1  \n",
       "98170   #charliehebdo_paris_attack-20150107_072714-201...           1  \n",
       "8782    #chapelhillshooting_were_media-20150211_142604...           0  \n",
       "191070  king_martin_luther-20150119_130240-20150119_13...           1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating final dataframe using randomly selected indices\n",
    "df = df_tweets.iloc[random_indices]\n",
    "\n",
    "# and taking a look\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dfeae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we give the tweet_ids in 'df' to the Twitter API using Tweepy, to pull the tweet text.\n",
    "# first we need to authenticate our request by providing tokens.\n",
    "# sign-up for Twitter API online and generate your own tokens and them to config.ini.\n",
    "# refer to config.ini here to get authentication details\n",
    "config = RawConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "bear_token = config['twitter']['bearer_token']\n",
    "\n",
    "api_key = config['twitter']['api_key']\n",
    "api_key_secret = config['twitter']['api_key_secret']\n",
    "\n",
    "access_token = config['twitter']['access_token']\n",
    "access_token_secret = config['twitter']['access_token_secret']\n",
    "\n",
    "# initialize call to Twitter API v2, with authentication\n",
    "client = tweepy.Client(\n",
    "    bearer_token= bear_token,\n",
    "    consumer_key= api_key,\n",
    "    consumer_secret= api_key_secret,\n",
    "    access_token= access_token,\n",
    "    access_token_secret= access_token_secret,\n",
    "    wait_on_rate_limit=True # set to true to ensure max of approx 10K tweets pulled per every 15 mins\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f1e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter API can only return tweets in batches of 100\n",
    "# define function to split list of tweet IDs into a nested format, ie, a list of 100-item lists\n",
    "\n",
    "def id_grouper(tweet_ids):\n",
    "    grouped_ids = []\n",
    "    for n in range(0, len(tweet_ids), 100):\n",
    "        grouped_ids.append([tweet_ids.iloc[iden] for iden in range(n, n+100)])\n",
    "        \n",
    "    return grouped_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "decad307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 802 seconds.\n",
      "Rate limit exceeded. Sleeping for 802 seconds.\n",
      "Rate limit exceeded. Sleeping for 806 seconds.\n",
      "Rate limit exceeded. Sleeping for 806 seconds.\n",
      "Rate limit exceeded. Sleeping for 806 seconds.\n",
      "Rate limit exceeded. Sleeping for 805 seconds.\n",
      "Rate limit exceeded. Sleeping for 804 seconds.\n",
      "Rate limit exceeded. Sleeping for 806 seconds.\n",
      "Rate limit exceeded. Sleeping for 808 seconds.\n",
      "Rate limit exceeded. Sleeping for 807 seconds.\n",
      "Rate limit exceeded. Sleeping for 807 seconds.\n",
      "Rate limit exceeded. Sleeping for 806 seconds.\n",
      "Rate limit exceeded. Sleeping for 806 seconds.\n",
      "Rate limit exceeded. Sleeping for 808 seconds.\n",
      "Rate limit exceeded. Sleeping for 807 seconds.\n",
      "Rate limit exceeded. Sleeping for 807 seconds.\n",
      "Rate limit exceeded. Sleeping for 807 seconds.\n",
      "Rate limit exceeded. Sleeping for 806 seconds.\n",
      "Rate limit exceeded. Sleeping for 807 seconds.\n",
      "Rate limit exceeded. Sleeping for 807 seconds.\n",
      "Rate limit exceeded. Sleeping for 807 seconds.\n",
      "Rate limit exceeded. Sleeping for 804 seconds.\n",
      "Rate limit exceeded. Sleeping for 807 seconds.\n",
      "Rate limit exceeded. Sleeping for 807 seconds.\n",
      "Rate limit exceeded. Sleeping for 808 seconds.\n",
      "Rate limit exceeded. Sleeping for 805 seconds.\n",
      "Rate limit exceeded. Sleeping for 805 seconds.\n",
      "Rate limit exceeded. Sleeping for 803 seconds.\n",
      "Rate limit exceeded. Sleeping for 803 seconds.\n",
      "Rate limit exceeded. Sleeping for 804 seconds.\n",
      "Rate limit exceeded. Sleeping for 803 seconds.\n",
      "Rate limit exceeded. Sleeping for 802 seconds.\n",
      "Rate limit exceeded. Sleeping for 801 seconds.\n",
      "Rate limit exceeded. Sleeping for 801 seconds.\n",
      "Rate limit exceeded. Sleeping for 801 seconds.\n",
      "Rate limit exceeded. Sleeping for 798 seconds.\n",
      "Rate limit exceeded. Sleeping for 789 seconds.\n",
      "Rate limit exceeded. Sleeping for 786 seconds.\n",
      "Rate limit exceeded. Sleeping for 786 seconds.\n",
      "Rate limit exceeded. Sleeping for 795 seconds.\n",
      "Rate limit exceeded. Sleeping for 796 seconds.\n",
      "Rate limit exceeded. Sleeping for 796 seconds.\n",
      "Rate limit exceeded. Sleeping for 794 seconds.\n",
      "Rate limit exceeded. Sleeping for 795 seconds.\n",
      "Rate limit exceeded. Sleeping for 794 seconds.\n",
      "Rate limit exceeded. Sleeping for 791 seconds.\n",
      "Rate limit exceeded. Sleeping for 794 seconds.\n",
      "Rate limit exceeded. Sleeping for 792 seconds.\n",
      "Rate limit exceeded. Sleeping for 791 seconds.\n"
     ]
    }
   ],
   "source": [
    "# loop to make repeated API calls.\n",
    "# note: will pause once rate limit is exceeded.\n",
    "# our dataset of 1.5M tweets took about 10 hours.\n",
    "# about half of the tweet_ids have been removed from Twitter.\n",
    "# as a result, text was fetched for about 50% of the 1.5 million we attempted.\n",
    "\n",
    "tweet_text = []\n",
    "\n",
    "for group in id_grouper(df.tweet_id):\n",
    "    \n",
    "    tweets = client.get_tweets(group)\n",
    "    for i in range(len(tweets.data)):\n",
    "        tweet_text.append([tweets.data[i].id, tweets.data[i].text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# many of these are retweets, and so contain duplicate content.\n",
    "# removing these leaves 344,208 unique tweets.\n",
    "df_tweet_texts = pd.DataFrame(tweet_text, columns=['tweet_id','text'])\n",
    "df_tweet_texts.drop_duplicates(inplace=True)\n",
    "\n",
    "# merging our text data with credibility scores\n",
    "df = pd.merge(df_tweets, df_tweet_texts, how='left', on='tweet_id').dropna()\n",
    "\n",
    "# exporting these as final \n",
    "df.to_csv('../datasets/final_train_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
